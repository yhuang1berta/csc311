# -*- coding: utf-8 -*-
"""csc311_A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XRRUfO4Z9siJV3Y5sjwfuCDSRIo6KOsj
"""

import pickle
import scipy.interpolate as interpolate
# q2
import sklearn.linear_model as lin
import sklearn.discriminant_analysis as dis
import sklearn.naive_bayes as NB
import scipy.stats as sta
# q3
import sklearn.neural_network as NN

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Oct 27 18:01:18 2017

@author: anthonybonner
"""


import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D


# Plot the decision boundaries of classifier clf
# and colour the decision regions.
# Assume three classes.
def boundaries(clf):
    
    # The extent of xy space
    ax = plt.gca()
    x_min,x_max = ax.get_xlim()
    y_min,y_max = ax.get_ylim()
     
    # form a mesh/grid over xy space
    h = 0.02    # mesh granularity
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    mesh = np.c_[xx.ravel(),yy.ravel()]
    
    # evaluate the decision function at the grid points
    Z = clf.predict(mesh)
    
    # Use pale colours for the decision regions.
    Z = Z.reshape(xx.shape)
    mylevels=[-0.5,0.5,1.5,2.5]
    ax.contourf(xx, yy, Z, levels=mylevels, colors=('red','blue','green'), alpha=0.2)
    
    # draw the decision boundaries in solid black
    ax.contour(xx, yy, Z, levels=3, colors='k', linestyles='solid')
    
        


# Plot the decision function of classifier clf in 3D.
# if Cflag=1 (the default), draw a contour plot of the decision function
# beneath the 3D plot.

def df3D(clf,cFlag=1):    
    
    # the extent of xy space
    ax = plt.gca()
    x_min,x_max = ax.get_xlim()
    y_min,y_max = ax.get_ylim()
    
    # form a mesh/grid over xy space
    h = 0.01    # mesh granularity
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    mesh = np.c_[xx.ravel(),yy.ravel()]
    
    # evaluate the decision function at the grid points
    Z = clf.predict_proba(mesh)[:,1]
    
    # plot the contours of the decision function
    Z = Z.reshape(xx.shape)
    ax.plot_surface(xx, yy, Z, cmap=cm.RdBu, linewidth=0, rcount=75, ccount=75)
    mylevels=np.linspace(0.0,1.0,11)
    ax.contour(xx,yy,Z,levels=mylevels,linestyles='solid',linewidths=3,cmap=cm.RdBu)
    
    # limits of the vertical axis
    z_min = 0.0
    z_max = 1.0
    
    if cFlag == 1:
        # display a contour plot of the decision function
        z_min = -0.5
        ax.contourf(xx, yy, Z, levels=mylevels, cmap=cm.RdBu, offset=z_min)
    
    ax.set_zlim(z_min,z_max)

with open("dataA2Q1.pickle","rb") as file:
  dataTrain, dataTest = pickle.load(file)

def train_w(dataTrain, dataTest, K):
  # initializing phi(x) for training data
  z_train = np.arange(K+1)
  z_train = np.insert(z_train,z_train.shape[0],z_train[1:])
  z_train = z_train.reshape((1,z_train.shape[0]))
  z_train = np.insert(z_train,[1]*(dataTrain.shape[1]-1),z_train[0],axis=0)
  z_train = z_train * dataTrain[0].reshape((dataTrain.shape[1],1))
  z_train[:,1:K+1] = np.sin(z_train[:,1:K+1])
  z_train[:,K+1:] = np.cos(z_train[:,K+1:])
  z_train[:,0] = np.ones(dataTrain.shape[1])
  # print(z_train)
  # initializing phi(x) for test data
  z_test = np.arange(K+1)
  z_test = np.insert(z_test,z_test.shape[0],z_test[1:])
  z_test = z_test.reshape((1,z_test.shape[0]))
  z_test = np.insert(z_test,[1]*(dataTest.shape[1]-1),z_test[0],axis=0)
  z_test = z_test * dataTest[0].reshape((dataTest.shape[1],1))
  z_test[:,1:K+1] = np.sin(z_test[:,1:K+1])
  z_test[:,K+1:] = np.cos(z_test[:,K+1:])
  z_test[:,0] = np.ones(dataTest.shape[1])

  # minimize least squares to find w
  w, _, _, _ = np.linalg.lstsq(z_train, dataTrain[1], rcond=None)

  # find training error and test error using w
  p_train = z_train @ w
  p_test = z_test @ w
  err_train = np.sum(np.square(dataTrain[1,:]-p_train))/dataTrain.shape[1]
  err_test = np.sum(np.square(dataTest[1,:] - p_test))/dataTest.shape[1]

  return w, p_train, p_test, err_train, err_test

def fit_plot(dataTrain, dataTest, K):
  # get w
  w, p_train, p_test, err_train, err_test = train_w(dataTrain, dataTest, K)

  # plot graph
  plt.scatter(dataTrain[0],dataTrain[1],20)
  x_new = np.linspace(np.amin(dataTrain[0]), np.amax(dataTrain[0]),1000)
  a_BSpline = interpolate.make_interp_spline(dataTrain[0,dataTrain[0,:].argsort()],p_train[dataTrain[0,:].argsort()])
  y_new = a_BSpline(x_new)
  plt.plot(x_new,y_new,color="red")
  plt.ylim(np.amin(dataTrain[1])-5, np.amax(dataTrain[1])+5)
  plt.show()



  return w, err_train, err_test

def q1_plot(K, sub_q):
  print('\nQuestion 1('+sub_q+'):')
  plt.suptitle("Question 1("+sub_q+"): the fitted function (K="+str(K)+")")
  plt.xlabel("x")
  plt.ylabel("y")
  w, err_train, err_test = fit_plot(dataTrain, dataTest, K)
  print(err_train)
  print(err_test)
  print(w)

print('\n\nQuestion 1')
print('----------')
q1_plot(3,'b')
q1_plot(9,'c')
q1_plot(12,'d')

def q1_e():
  
  # set plot size to increase resolution
  plt.figure(figsize=(18,12))
  
  # loop over all values for k and plot the graph
  for i in range(1,13):
    plt.subplot(4,3,i)
    _, p_train, _, _, _ = train_w(dataTrain, dataTest, i)
    plt.scatter(dataTrain[0],dataTrain[1],20)
    x_new = np.linspace(np.amin(dataTrain[0]), np.amax(dataTrain[0]),1000)
    a_BSpline = interpolate.make_interp_spline(dataTrain[0,dataTrain[0,:].argsort()],p_train[dataTrain[0,:].argsort()])
    y_new = a_BSpline(x_new)
    plt.plot(x_new,y_new,color="red")
    plt.ylim(np.amin(dataTrain[1])-5, np.amax(dataTrain[1])+5)

  plt.suptitle("Question 1(e): fitted functions for many values of K")
  plt.show()

print("Question 1(e):")
q1_e()

def cross_validate(dataTrain):
  i = dataTrain.shape[1]//5 # which is 5 in our case
  min_mean_val_err = float("inf")
  best_K = 1
  err_train_list, err_val_list = [], [] # lists to store mean training error and mean validation error

  for k in range(1,13):
    mean_train_err, mean_val_err = 0, 0
    for n in range(5):
      _, _, _, err_train, err_val = train_w(np.delete(dataTrain, np.arange(n*i,(n+1)*i),axis=1), dataTrain[:,n*i:(n+1)*i], k)
      mean_train_err += err_train
      mean_val_err += err_val
    mean_train_err /= 5
    mean_val_err /= 5
    err_train_list.append(mean_train_err)
    err_val_list.append(mean_val_err)
    if mean_val_err < min_mean_val_err:
      min_mean_val_err = mean_val_err
      best_K = k

  # plot graph
  plt.suptitle("Question 1(f): mean training and validation error")
  plt.semilogy()
  plt.xlabel("K")
  plt.ylabel("Mean error")
  Ks = np.arange(1,13)
  plt.plot(Ks,err_train_list,color="blue")
  plt.plot(Ks,err_val_list,color="red")
  plt.show()

  # repeat part (b)
  plt.suptitle("Question 1(f): the best fitting")
  plt.xlabel("x")
  plt.ylabel("y")
  w, err_train, err_test = fit_plot(dataTrain, dataTest, best_K)
  print(err_train)
  print(err_test)
  print(w)

print("Question 1(f):")
cross_validate(dataTrain)

with open("dataA2Q2.pickle", "rb") as file:
  dataTrain, dataTest = pickle.load(file)
Xtrain, Ttrain = dataTrain
Xtest, Ttest = dataTest

# Question 2(a):
def plot_data(X,T):
  plt.scatter(X[T==0][:,0],X[T==0][:,1],s=2,c="red")
  plt.scatter(X[T==1][:,0],X[T==1][:,1],s=2,c="blue")
  plt.scatter(X[T==2][:,0],X[T==2][:,1],s=2,c="green")
  plt.xlim(np.amin(Xtrain[:,0])-0.1,np.amax(Xtrain[:,0])+0.1)
  plt.ylim(np.amin(Xtrain[:,1])-0.1,np.amax(Xtrain[:,1])+0.1)

# Question 2(b):
clf = lin.LogisticRegression(multi_class="multinomial",solver="lbfgs")
clf.fit(Xtrain, Ttrain)

# find accuracy1 by calling score
accuracy1 = clf.score(Xtest, Ttest)

def accuracyLR(clf, X, T):
  # used to find accuracy manually
  prediction = (clf.coef_ @ X.T).T+clf.intercept_
  softmax_prediction = np.exp(prediction)/np.sum(np.exp(prediction),axis=1).reshape((prediction.shape[0],1))
  predicted_val = np.argmax(softmax_prediction,axis=1) == T
  accuracy = np.count_nonzero(predicted_val == True)/T.shape[0]
  return accuracy

# find accuracy2 by calling accuracyLR
accuracy2 = accuracyLR(clf, Xtest, Ttest)

print('\n\nQuestion 2')
print('----------')
print("Question 2(b):")
print(accuracy1)
print(accuracy2)
print(accuracy1-accuracy2)

# plot data
plt.suptitle("Question 2(b): decision boundaries for logistic regression")
plot_data(Xtrain, Ttrain)
boundaries(clf)

# Question 2(c):
clf = dis.QuadraticDiscriminantAnalysis(store_covariance=True)
clf.fit(Xtrain, Ttrain)

# find accuracy 1
accuracy1 = clf.score(Xtest, Ttest)

def accuracyQDA(clf,X,T):
  prediction = np.zeros((X.shape[0], clf.means_.shape[0]))
  for i in range(clf.means_.shape[0]):
    y = sta.multivariate_normal.pdf(x=X, mean=clf.means_[i], cov=clf.covariance_[i]) * clf.priors_[i]
    prediction[:,i] = y
  
  predicted_val = np.argmax(prediction,axis=1) == T
  accuracy = np.count_nonzero(predicted_val == True)/T.shape[0]
  return accuracy

# find accuracy 2
accuracy2 = accuracyQDA(clf, Xtest, Ttest)

print("Question 2(c):")
print(accuracy1)
print(accuracy2)
print(accuracy1-accuracy2)

# plot data
plt.suptitle("Question 2(c): decision boundaries for quadratic discriminant analysis")
plot_data(Xtrain, Ttrain)
boundaries(clf)

def accuracyNB(clf, X, T):
  broad_X = X.reshape((X.shape[0],1,X.shape[1]))
  broad_theta = clf.theta_.reshape((1,clf.theta_.shape[0],clf.theta_.shape[1]))
  expo = -(broad_X-broad_theta)**2/(2*clf.sigma_)
  total_x_prob = np.prod(np.exp(expo)/(np.sqrt(2*np.pi)*np.sqrt(clf.sigma_)),axis=2) * clf.class_prior_
  est = np.argmax(total_x_prob, axis=1) == T
  acc = np.count_nonzero(est == True)/T.shape[0]
  return acc

# Question 2(d):
clf = NB.GaussianNB()
clf.fit(Xtrain, Ttrain)
# find accuracy 1
accuracy1 = clf.score(Xtrain, Ttrain)
# find accuracy 2
accuracy2 = accuracyNB(clf, Xtrain, Ttrain)
print("Question 2(d):")
print(accuracy1)
print(accuracy2)
print(accuracy1-accuracy2)
# plot data
plt.suptitle("Question 2(d): decision boundaries for Gaussian naive Bayes")
plot_data(Xtrain, Ttrain)
boundaries(clf)

# Question 3(a):
with open("dataA2Q2.pickle", "rb") as file:
  dataTrain, dataTest = pickle.load(file)
Xtrain, Ttrain = dataTrain
Xtest, Ttest = dataTest

# Question 3(b):
np.random.seed(0)

# initialize and train neural nets
def fit_then_plot(hidden_unit,activation,solver,alpha,tol,max_iter):
  mlp = NN.MLPClassifier(hidden_layer_sizes=(hidden_unit),activation=activation,solver=solver,alpha=alpha,tol=tol,max_iter=max_iter)
  mlp.fit(Xtrain, Ttrain)
  accuracy = mlp.score(Xtest, Ttest)
  print(accuracy)

  # plot data
  plot_data(Xtrain, Ttrain)
  boundaries(mlp)

print("\nQuestion 3:")
print("--------------")
plt.suptitle("Question 3(b): Neural net with 1 hidden unit")
fit_then_plot(1,"logistic","sgd",0.01,10**(-6),1000)

# Question 3(c):
print("Questioin 3(c):")
plt.suptitle("Question 3(c): Neural net with 2 hidden units")
fit_then_plot(2,"logistic","sgd",0.01,10**(-6),1000)

# Question 3(d):
print("Question 3(d):")
plt.suptitle("Question 3(d): Neural net with 9 hidden units")
fit_then_plot(9,"logistic","sgd",0.01,10**(-6),1000)

# Question 3(e):
# note: this code is written using google colab, where full-screen mode is not available, therefore setting resolution manually is necessary
plt.figure(figsize=(18,12))
for i in range(2,11):
    plt.subplot(3,3,i-1)
    fit_then_plot(7,"logistic","sgd",0.01,10**(-6),2**i)

plt.suptitle("Question 3(e): different numbers of epochs")
plt.show()

# Question 3(f):
# There are 6 lines of code as explained in Q3(e)
plt.figure(figsize=(18,12))
for i in range(2,11):
    plt.subplot(3,3,i-1)
    fit_then_plot(5,"logistic","sgd",i-1,10**(-6),2**i)

plt.suptitle("Question 3(f): different initial weights")
plt.show()

# Question 3(g):
np.random.seed(0)
mlp = NN.MLPClassifier(hidden_layer_sizes=9,activation="logistic",solver="sgd",alpha=0.01,tol=10**-6,max_iter=1000)
mlp.fit(Xtrain, Ttrain)

# find accuracy1
accuracy1 = mlp.score(Xtest, Ttest)

def accuracyNN(clf,X,T):
  # turn clf.coefs_ into numpy array
  np_coefs = np.array(clf.coefs_)

  # find maginal probability through broadcasting
  z1 = X[:,None,:] * np_coefs[0].T[None,:,:]

  # find z1 by summing up all conditional probability and add bias
  z1 = np.sum(z1,axis=2) + clf.intercepts_[0]

  # helper function to compute sigmoid
  def sigmoid(z):
    return 1/(1+np.exp(-z))

  # run hidden layer 1 through activation
  y1 = sigmoid(z1)

  # find maginal probability through broadcasting
  z2 = y1[:,None,:] * np_coefs[1].T[None,:,:]

  # find z2 by summing up all class-conditional probabilities and add bias
  z2 = np.sum(z2, axis=2) + clf.intercepts_[1]

  # run z2 through softmax
  y2 = (np.exp(z2).T / (np.sum(np.exp(z2),axis=1))).T

  # find largest probability for each sample
  prediction = np.argmax(y2,axis=1)

  # check with T
  verify = prediction == T

  # compute accuracy
  acc = np.mean(verify)

  return acc

# find accuracy 2
accuracy2 = accuracyNN(mlp, Xtest, Ttest)

print("Question 3(h):")
print(accuracy1)
print(accuracy2)
print(accuracy1 - accuracy2)

# Question 3(h):
np.random.seed(0)
mlp = NN.MLPClassifier(hidden_layer_sizes=9,activation="logistic",solver="sgd",alpha=0.01,tol=10**-6,max_iter=1000)
mlp.fit(Xtrain, Ttrain)

# turn Ttest into one-hot-encoding
one_hot_T = np.zeros((Ttest.shape[0],Ttest.max()+1))
one_hot_T[np.arange(Ttest.shape[0]),Ttest]=1

def ceNN(clf,X,one_hot_T):
  # turn clf.coefs_ into numpy array
  np_coefs = np.array(clf.coefs_)

  # find maginal probability through broadcasting
  z1 = X[:,None,:] * np_coefs[0].T[None,:,:]

  # find z1 by summing up all conditional probability and add bias
  z1 = np.sum(z1,axis=2) + clf.intercepts_[0]

  # helper function to compute sigmoid
  def sigmoid(z):
    return 1/(1+np.exp(-z))

  # run hidden layer 1 through activation
  y1 = sigmoid(z1)

  # find maginal probability through broadcasting
  z2 = y1[:,None,:] * np_coefs[1].T[None,:,:]

  # find z2 by summing up all class-conditional probabilities and add bias
  z2 = np.sum(z2, axis=2) + clf.intercepts_[1]

  # run z2 through softmax
  y2 = (np.exp(z2).T / (np.sum(np.exp(z2),axis=1))).T

  # log y2 element-wise
  logy = np.log(y2)

  # compute CE1
  CE1 = np.sum(clf.predict_log_proba(X) * -one_hot_T)/one_hot_T.shape[0]

  # compute CE2
  CE2 = logy * -one_hot_T
  CE2 = np.sum(CE2)/one_hot_T.shape[0]

  return CE1,CE2

# find CE2
CE1,CE2 = ceNN(mlp, Xtest, one_hot_T)

print("Question 3(g):")
print(CE1)
print(CE2)
print(CE1-CE2)

# Question 5(a):
with open('mnistTVT.pickle','rb') as f:
  Xtrain,Ttrain,Xval,Tval,Xtest,Ttest = pickle.load(f)

def reduce_train(Xtrain,Ttrain,c1,c2):
  reduced_Ttrain_index = np.where((Ttrain == c1) | (Ttrain == c2), True, False)
  full_reduced_Xtrain = Xtrain[reduced_Ttrain_index]
  full_reduced_Ttrain = Ttrain[reduced_Ttrain_index]
  full_reduced_Ttrain = full_reduced_Ttrain == c1
  full_reduced_Ttrain = full_reduced_Ttrain.astype(int)
  return full_reduced_Xtrain, full_reduced_Ttrain

# initialize reduced Train/Test data
reduced_Xtrain, reduced_Ttrain = reduce_train(Xtrain,Ttrain,5,6)
reduced_Xtest, reduced_Ttest = reduce_train(Xtest,Ttest,5,6)
debug_Xtrain,debug_Ttrain = reduce_train(Xtrain,Ttrain,4,5)

# For the rest of Question 5
# I don't know

# #Question 5(b):
# clf = NN.MLPClassifier(hidden_layer_sizes=(100,100),activation="tanh",solver="sgd",tol=10**-6)
# clf.fit(reduced_Xtrain,reduced_Ttrain)

# def evaluateNN(clf,X,T):

#   # helper function to compute sigmoid
#   def sigmoid(z):
#     return 1/(1+np.exp(-z))

#   # helper function to compute tanh
#   def tanh(z):
#     return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))

#   # find acc1
#   acc1 = clf.score(X,T)

#   # turn clf.coefs_ into numpy array
#   np_coefs = np.array(clf.coefs_)

#   # find marginal probability through broadcasting
#   z1 = X[:,None,:] * np_coefs[0].T[None,:,:]
#   # find z1 by summing up all conditional probability and add bias
#   z1 = np.sum(z1,axis=2) + clf.intercepts_[0]
#   # run hidden layer 1 through activation
#   y1 = tanh(z1)

#   # find marginal probability through broadcasting
#   z2 = y1[:,None,:] * np_coefs[1].T[None,:,:]
#   # find z1 by summing up all conditional probability and add bias
#   z2 = np.sum(z2,axis=2) + clf.intercepts_[1]
#   # run hidden layer 1 through activation
#   y2 = tanh(z2)

#   # find marginal probability through broadcasting
#   g = y2[:,None,:] * np_coefs[2].T[None,:,:]
#   # find z2 by summing up all class-conditional probabilities and add bias
#   g = np.sum(g, axis=2) + clf.intercepts_[2]
#   # run z2 through softmax
#   o = sigmoid(g)

#   # find largest probability for each sample
#   prediction = (o>(1/2)).astype(int)

#   # check with T
#   verify = prediction == T

#   # compute accuracy
#   acc2 = np.mean(verify)

#   return acc1, acc2



