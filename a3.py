# -*- coding: utf-8 -*-
"""csc311_A3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GRYQN4DQJ4BtTSbixiKl4LcHUWmBTHaw
"""

import numpy as np
import pickle
import sklearn.decomposition
import sklearn.discriminant_analysis
import sklearn.cluster
import sklearn.mixture
from sklearn.utils.testing import ignore_warnings
import sklearn.utils
import warnings
import matplotlib.pyplot as plt

with open("mnistTVT.pickle","rb") as f:
  Xtrain, Ttrain, Xval, Tval, Xtest, Ttest = pickle.load(f)

Xtrain = Xtrain[0:5000,:].astype(np.float64)
Ttrain = Ttrain[0:5000].astype(np.float64)
Xval = Xval[0:5000,:].astype(np.float64)
Tval = Tval[0:5000].astype(np.float64)
Xtest = Xtest[0:5000,:].astype(np.float64)
Ttest = Ttest[0:5000].astype(np.float64)

print("*** Using only 5,000 MNIST training points ***")

def reduce_data(n,svd):
  if svd:
    pca = sklearn.decomposition.PCA(n,svd_solver=svd)
  else:
    pca = sklearn.decomposition.PCA(n)
  pca.fit(Xtrain)
  reduced = pca.transform(Xtrain)
  projected = pca.inverse_transform(reduced)
  return pca, reduced, projected

def plot_first_25(data,part,dimension,addition):
  for i in range(25):
    plt.subplot(5,5,i+1)
    plt.axis(False)
    plt.imshow(projected[i].reshape((28,28)),cmap="Greys",interpolation="nearest")
  plt.suptitle("Question 1("+part+"): MNIST test data projected onto "+str(dimension)+" dimensions"+addition)
  plt.plot()

print('\n\nQuestion 1')
print('----------')
print('\nQuestion 1(a):')
_, _, projected = reduce_data(30,None)
plot_first_25(projected,'a',30,'')

print('\nQuestion 1(b):')
_,_,projected = reduce_data(3,None)
plot_first_25(projected,'b',3,'')

print('\nQuestion 1(c):')
_,reduced,projected = reduce_data(300,None)
plot_first_25(projected,'c',300,'')

def myPCA(X,K):
  # find mean and covariance matrix of X
  mean = np.mean(X.T,axis=1)
  cov = np.dot((X-mean[None,:]).T,X-mean[None,:])/X.shape[0]
  # find eigenvalues and eigenvectors of X
  eig_val, eig_vec = np.linalg.eigh(cov)
  # construct U
  U = -eig_vec[:,(-eig_val).argsort()][:,:K]
  # find X's projection basis -- z
  Z = (U.T@(X.T-mean[:,None]))
  # reconstruct Xp with new basis
  Xp = mean[None,:]+(U@Z).T

  return Xp

# temp = np.array([[90,60,90],[90,90,30],[60,60,60],[60,60,90],[30,30,30]])
# myPCA(temp,2)
# projected = myPCA(Xtrain,300)
# plot_first_25(projected,'a',300)

# Q1(e).
print('\nQuestion 1(f):')
myXtrainP = myPCA(Xtrain,100)
_,_,XtrainP = reduce_data(100,"full")

plot_first_25(myXtrainP,'f',100,"(mine)")

plot_first_25(XtrainP,'f',100,"(sklearn)")

RMS = np.sqrt(np.sum(np.square((XtrainP-myXtrainP))/XtrainP.shape[0]))
print(RMS)

# Q2(a).
print('\n\nQuestion 2')
print('----------')
print('\nQuestion 2(a):')
X = Xtrain[:200]
T = Ttrain[:200]
debugX = Xtrain[:300]
debugT = Ttrain[:300]

def Q2a():
  warnings.filterwarnings("ignore", category=UserWarning)
  clf = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis()
  clf.fit(X,T)
  acc = clf.score(Xtest, Ttest)
  print(acc)

Q2a()

# Q2(b).
print('\nQuestion 2(b):')
def Q2b():
  # initialize
  warnings.filterwarnings("ignore", category=UserWarning)
  val_acc = []
  train_acc = []
  best_acc = -1
  best_acc_n = 0
  # find best regularization term
  for n in range(21):
    clf = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param=2**(-n))
    clf.fit(X,T)
    train_acc.insert(0,clf.score(X,T))
    acc = clf.score(Xval,Tval)
    if acc > best_acc:
      best_acc = acc
      best_acc_n = n
    val_acc.insert(0,acc)
  print('%.4f' %best_acc)
  print(best_acc_n)

  # plot train accuarcy vs regularization term
  # print(val_acc)
  # val_acc = val_acc[::-1]
  # print(val_acc)
  # train_acc = train_acc[::-1]
  plt.suptitle("Question 2(b): Training and Validation Accuracy for Regularized QDA")
  plt.semilogx()
  plt.xlabel("Regularization parameter")
  plt.ylabel("Accuracy")
  Ks = np.power(np.ones(21)*2,-(np.arange(20,-1,-1)))
  plt.plot(Ks,train_acc,color="blue")
  plt.plot(Ks,val_acc,color="red")
  # plt.gca().invert_xaxis()
  plt.show()

Q2b()

# Q2(d).
def train2d(K,X,T):
  # reduce X using PCA
  pca = sklearn.decomposition.PCA(K,svd_solver="full")
  pca.fit(X,T)
  reduced = pca.transform(X)

  # train QDA using reduced X
  warnings.filterwarnings("ignore", category=UserWarning)
  clf = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis()
  
  clf.fit(reduced,T)
  acc = clf.score(reduced,T)

  return pca, clf, acc

def test2d(pca,qda,X,T):
  reduced = pca.transform(X)
  return qda.score(reduced, T)

print('\nQuestion 2(d):')
def Q2d():
  # initialize
  val_acc = []
  train_acc = []
  best_val_acc = -1
  val_K = 1
  val_train_acc = -1
  # get validation accuracy
  for K in range(1,51):
    pca, clf, acc = train2d(K,X,T)
    score = test2d(pca,clf,Xval,Tval)
    if best_val_acc < score:
      best_val_acc = score
      val_K = K
      val_train_acc = acc
    val_acc.append(score)
    train_acc.append(acc)
  # output maximum validation accuracy
  print(best_val_acc)
  print(val_K)
  print(val_train_acc)
  # plot graph
  plt.suptitle("Question 2(d): Training and Validation Accuracy for PCA + QDA")
  plt.xlabel("Reduced dimension")
  plt.ylabel("Accuracy")
  Ks = np.arange(1,51)
  plt.plot(Ks,train_acc,color="blue")
  plt.plot(Ks,val_acc,color="red")
  # plt.gca().invert_xaxis()
  plt.show()

Q2d()

# Q2(f).
def Q2f():
  # initialize
  warnings.filterwarnings("ignore", category=UserWarning)
  accMax = -1
  train_acc = -1
  best_K = 1
  best_reg = 0
  max_k = []
  # reduce X by various K
  for K in range(1,51):
    pca = sklearn.decomposition.PCA(K)
    pca.fit(X,T)
    reduced = pca.transform(X)
    accMaxK = -1
    # train clf on reduced X with various regularization parameter
    for n in range(21):
      clf = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param=2**(-n))
      clf.fit(reduced,T)
      acc = clf.score(pca.transform(Xval),Tval)
      if acc > accMaxK:
        accMaxK = acc
      if acc > accMax:
        accMax = acc
        best_reg = n
        best_K = K
        train_acc = clf.score(reduced, T)
    max_k.append(accMaxK)
  # output results
  print(accMax)
  print(train_acc)
  print(2**-(best_reg))
  print(best_K)
  # plot graph
  plt.suptitle("Question 2(f): Maximum validation accuracy for QDA")
  plt.xlabel("Reduced dimension")
  plt.ylabel("maximum accuracy")
  Ks = np.arange(1,51)
  plt.plot(Ks,max_k)
  # plt.gca().invert_xaxis()
  plt.show()

print('\nQuestion 2(f):')
Q2f()

# Q3(a).
def myBootstrap(X,T):
  t_count = np.array([1])
  while any(t_count<3):
    sample = sklearn.utils.resample(X,T)
    _,t_count = np.unique(np.array(sample[1]),return_counts=True)
  # sample[0] = np.array(sample[0])
  # # turning Tsample to one hot encoding
  # sample[1] = np.array(sample[1]).astype(np.int)
  # temp = np.zeros((sample[1].size, sample[1].max()+1))
  # temp[np.arange(sample[1].size),sample[1]] = 1
  # sample[1] = temp
  return sample

# Q3(b).
def Q3b():
  warnings.filterwarnings("ignore", category=UserWarning)
  # train base classifier
  base = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param=0.004)
  base.fit(X,T)
  print("validation accuracy of the base classifier: " + str(base.score(Xval, Tval)))

  # train classifiers using bootstrap
  mat_list = []
  for _ in range(50):
    sample = myBootstrap(X,T)
    clf = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param=0.004)
    clf.fit(sample[0],sample[1])
    mat_list.append(clf.predict_proba(Xval))
  avg_prob = np.mean(mat_list,axis=0)
  bag_acc = np.mean(Tval == avg_prob.argsort()[:,-1])
  print("validation accuracy of the bagged classifier: " + str(bag_acc))

print('\n\nQuestion 3')
print('----------')
print('\nQuestion 3(b):')
Q3b()

# Q3c
def Q3c():
  # initialize
  warnings.filterwarnings("ignore", category=UserWarning)
  acc_list = []
  mat_list = []
  # train 500 classifiers with bootstrap
  for _ in range(500):
    sample = myBootstrap(X,T)
    clf = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param=0.004)
    clf.fit(sample[0],sample[1])
    mat_list.append(clf.predict_proba(Xval))
    avg_prob = np.mean(mat_list,axis=0)
    acc_list.append(np.mean(Tval == avg_prob.argsort()[:,-1]))
  # plot graph
  plt.suptitle("Question 3(c): Validation accuracy")
  plt.xlabel("Number of bootstrap samples")
  plt.ylabel("Accuracy")
  Ks = np.arange(1,501)
  plt.plot(Ks,acc_list)
  plt.show()
  # plot log-scaled graph
  plt.suptitle("Question 3(c): Validation accuracy (log scale)")
  plt.semilogx()
  plt.xlabel("Number of bootstrap samples(log-scaled)")
  plt.ylabel("Accuracy")
  Ks = np.arange(1,501)
  plt.plot(Ks,acc_list)
  # plt.gca().invert_xaxis()
  plt.show()

print('\nQuestion 3(c):')
Q3c()

# Q3d.
def train3d(K,R,X,T):
  # reduce X to dimension K using PCA
  pca = sklearn.decomposition.PCA(K)
  pca.fit(X,T)
  reduced = pca.transform(X)
  # train QDA with reduced X
  clf = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param=R)
  clf.fit(reduced,T)
  
  return pca, clf

def proba3d(pca,qda,X):
  # reduce X
  reduced = pca.transform(X)
  # compute probability matrix
  prob_mat = qda.predict_proba(reduced)
  return prob_mat

#Q3(e).
def myBag(K,R):
  # get pca and qda from helper
  pca, clf = train3d(K,R,X,T)
  # reduce validation data
  reduced_Xval = pca.transform(Xval)
  # compute accuracy of validation data
  base_acc = clf.score(reduced_Xval, Tval)

  # initialize
  mat_list = []
  # bag clf 200 times
  for _ in range(200):
    sample = myBootstrap(debugX,debugT)
    pca1, clf1 = train3d(K,R,sample[0],sample[1])
    mat_list.append(proba3d(pca1,clf1,Xval))
  ensembled_prob = np.mean(mat_list,axis=0)
  ensembled_acc = np.mean(Tval == ensembled_prob.argsort()[:,-1])

  return base_acc, ensembled_acc

#Q3(f).
print('\nQuestion 3(e):')
base_acc, bagged_acc = myBag(100,0.01)
print(base_acc)
print(bagged_acc)

# Q3(g).
def Q3g():
  # initialize
  base_list = []
  bagged_list = []
  # get accuracy of both classifiers with random K,R
  for _ in range(50):
    K = np.random.randint(1,11)
    R = np.random.uniform(0.2,1)
    base_acc, bagged_acc = myBag(K,R)
    base_list.append(base_acc)
    bagged_list.append(bagged_acc)
  # plot graph
  plt.suptitle("Question 3(g): Bagged v.s. base validation accuracy.")
  plt.xlabel("Base validation accuracy")
  plt.ylabel("Bagged validation accuracy")
  plt.xlim(0,1)
  plt.ylim(0,1)
  plt.scatter(base_list,bagged_list,color="blue")
  plt.plot([0,1],[0,1],color="red")
  plt.show()

print('\nQuestion 3(g):')
Q3g()

# Q3(h).
def Q3h():
  # initialize
  base_list = []
  bagged_list = []
  # get accuracy of both classifiers with random K,R
  for _ in range(50):
    K = np.random.randint(50,201)
    R = np.random.uniform(0,0.05)
    base_acc, bagged_acc = myBag(K,R)
    base_list.append(base_acc)
    bagged_list.append(bagged_acc)
  best_acc = max(bagged_list)
  print(best_acc)
  # plot graph
  plt.suptitle("Question 3(h): Bagged v.s. base validation accuracy.")
  plt.xlabel("Base validation accuracy")
  plt.ylabel("Bagged validation accuracy")
  plt.ylim(0,1)
  plt.scatter(base_list,bagged_list)
  plt.axhline(best_acc, color="red")
  plt.show()

print('\nQuestion 3(h):')
Q3h()

with open("dataA2Q2.pickle", "rb") as file:
  dataTrain, dataTest = pickle.load(file)
Xtrain, Ttrain = dataTrain
Xtest, Ttest = dataTest

# Q4(a).
def plot_clusters(X,R,Mu):
  # sort R
  R_sum = np.sum(R,axis=0)
  sorted_R = R[:,R_sum.argsort()]
  # print(sorted_R)
  # plot clusters
  plt.scatter(X[:,0],X[:,1],color=sorted_R,s=5)
  # plot cluster centers
  plt.scatter(Mu[:,0],Mu[:,1],color="black")
  plt.show()

#Q4(b).
def Q4b():
  # find clusters using Kmeans
  kmeans = sklearn.cluster.KMeans(3)
  R = kmeans.fit_predict(Xtrain)
  Mu = kmeans.cluster_centers_
  # turn R into one-hot encoding
  temp = np.zeros((R.shape[0], Mu.shape[0]))
  temp[np.arange(R.shape[0]),R] = 1
  R = temp
  # plot clusters
  plt.suptitle("Question 4(b): K means.")
  plot_clusters(Xtrain,R,Mu)
  # find accuracy
  train_acc = kmeans.score(Xtrain)
  test_acc = kmeans.score(Xtest)
  print(train_acc)
  print(test_acc)
  return train_acc, test_acc

print('\n\nQuestion 4')
print('----------')
print('\nQuestion 4(b):')
train_acc, test_acc = Q4b()

#Q4(c).
def Q4c():
  # find clusters using Gaussian Mixture
  GM = sklearn.mixture.GaussianMixture(3,covariance_type="spherical")
  GM.fit(Xtrain)
  R = GM.predict_proba(Xtrain)
  Mu = GM.means_
  # plot clusters
  plt.suptitle("Question 4(c): Gaussian mixture model(spherical)")
  plot_clusters(Xtrain,R,Mu)
  # compute accuracy
  train_acc = GM.score(Xtrain,Ttrain)
  test_acc = GM.score(Xtest,Ttest)
  print(train_acc)
  print(test_acc)
  return train_acc, test_acc

print('\nQuestion 4(c):')
_, q4c_test = Q4c()

# Q4(d).
def Q4d():
  # find clusters using Gaussian Mixture
  GM = sklearn.mixture.GaussianMixture(3,covariance_type="full")
  GM.fit(Xtrain)
  R = GM.predict_proba(Xtrain)
  Mu = GM.means_
  # plot clusters
  plt.suptitle("Question 4(d): Gaussian mixture model(full)")
  plot_clusters(Xtrain,R,Mu)
  # compute accuracy
  train_acc = GM.score(Xtrain,Ttrain)
  test_acc = GM.score(Xtest,Ttest)
  print(train_acc)
  print(test_acc)
  return train_acc, test_acc

print('\nQuestion 4(d):')
_, q4d_test = Q4d()
print("Q4d-Q4c test scores ="+str(q4d_test-q4c_test))

# Q4e.
def myKmeans(X,K,I):
  # initialize centroids to random data points in X, so centroids are close enough to the data points
  # without replacement to avoid repeition
  index = np.random.choice(X.shape[0],K,replace=False)
  Mu = X[index]
  # initialize
  R = np.zeros((X.shape[0],K))
  score_list = []
  
  for i in range(I):
  # Assignment step:
    # broadcast X and Mu
    X_3d = X[:,None,:]
    X_3d = np.tile(X_3d,(K,1))
    Mu_3d = np.tile(Mu,(X.shape[0],1))
    Mu_3d = Mu_3d.reshape(X.shape[0],K,X.shape[1])
    # calculate distance using pythagorean theorem
    squared_diff = np.power(X_3d-Mu_3d,2)
    dist_mat = squared_diff[:,:,0]+squared_diff[:,:,1]
    # find R based on distance (hard K-means)
    R = np.zeros((X.shape[0],K))
    R[np.arange(X.shape[0]),dist_mat.argsort()[:,0]] = 1
    R_mask = R.astype(np.bool)
    # record score
    score_list.append(np.sum(dist_mat[R_mask]))

  # Refitting step:
    # find the location of the new centroids
    dist_sum = R_mask.T@X
    R_sum = np.sum(R_mask,axis=0)
    Mu = dist_sum/R_sum[:,None]

  return Mu, R, score_list[:20]

def scoreKmeans(X,Mu):
  # broadcast X and Mu
  X_3d = X[:,None,:]
  X_3d = np.tile(X_3d,(Mu.shape[0],1))
  Mu_3d = np.tile(Mu,(X.shape[0],1))
  Mu_3d = Mu_3d.reshape(X.shape[0],Mu.shape[0],X.shape[1])
  # calculate distance using pythagorean theorem
  squared_diff = np.power(X_3d-Mu_3d,2)
  dist_mat = squared_diff[:,:,0]+squared_diff[:,:,1]
  # find R based on distance (hard K-means)
  R = np.zeros((X.shape[0],Mu.shape[0]))
  R[np.arange(X.shape[0]),dist_mat.argsort()[:,0]] = 1
  R_mask = R.astype(np.bool)
  # record score
  return np.sum(dist_mat[R_mask])

def Q4e():
  # initialize
  Mu,R,score_list = myKmeans(Xtrain,3,100)
  # plot graph 1
  plt.suptitle("Question 4(e): score v.s. iteration (K means)")
  plt.xlabel("Iteration")
  plt.ylabel("Score")
  Iters = np.arange(1,21).astype(np.int)
  plt.plot(Iters,score_list)
  plt.show()

  # plot graph 2
  plt.suptitle("Question 4(e): Data clustered by K means")
  plot_clusters(Xtrain,R,Mu)

  # print train score and test score
  test = scoreKmeans(Xtest,Mu)
  print(scoreKmeans(Xtrain,Mu))
  print(test)
  print("Q4e+Q4b test scores =" + str(test_acc+test))

print('\nQuestion 4(e):')
Q4e()

#Q4f.
# I don't know

# def myGMM(X,K,I):
#   # initialize centers with k-means algorithm, so our centers are close to data points
#   # without replacement to avoid repetition
#   index = np.random.choice(X.shape[0],K,replace=False)
#   Mu = X[index]
#   # initialize
#   cov = np.identity(K)
#   prior = np.ones(K)*(1/K)
  
#   for i in range(I):
#     X_3d = X[:,None,:]
#     X_3d = np.tile(X_3d,(Mu.shape[0],1))
#     Mu_3d = np.tile(Mu,(X.shape[0],1))
#     Mu_3d = Mu_3d.reshape(X.shape[0],Mu.shape[0],X.shape[1])
#     R = 1/(2*np.pi)**(X.shape[1]/2) * np.abs(cov) * np.exp((-1/2)*(X_3d-Mu_3d).T*np.linalg.inv(cov)*(X_3d-Mu_3d))
#     break
#   print(R)

# K = 3
# X = Xtrain
# index = np.random.choice(X.shape[0],K,replace=False)
# Mu = X[index]
# cov = np.identity(K)
# prior = np.ones(K)*(1/K)

# X_3d = X[:,None,:]
# X_3d = np.tile(X_3d,(Mu.shape[0],1))
# Mu_3d = np.tile(Mu,(X.shape[0],1))
# Mu_3d = Mu_3d.reshape(X.shape[0],Mu.shape[0],X.shape[1])
# R = 1/(2*np.pi)**(X.shape[1]/2) * np.abs(cov) * np.exp((-1/2)*(X_3d-Mu_3d).T*np.linalg.inv(cov)*(X_3d-Mu_3d))
# print(R)

# a = np.arange(16).reshape((8,2))
# b = [2,6]
# print(a)
# print(np.split(a,b,axis=0))

# # np.square((XtrainP-myXtrainP))
# # print(XtrainP.shape)
# # print(myXtrainP.shape)
# x = np.arange(4)
# s = np.sum(x,axis=0)
# print(len(x.shape))

